{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt3tjDtpWX-A"
      },
      "source": [
        "## Agentic RAG Pipeline with memory and chat\n",
        "\n",
        "_code by Stefano Fiorucci ([Twitter](https://x.com/theanakin87), [LI](https://www.linkedin.com/in/stefano-fiorucci/))_\n",
        "\n",
        "Here is an agentic RAG pipelines that can remember previous chat messages. The most helpful assistants don't forget the thing you just said. 📓\n",
        "\n",
        "This pipeline takes advantage of Haystack's flexible looping capabilities, running and saving previous interactions until you exit.\n",
        "\n",
        "Components used:\n",
        "- [`InMemoryDocumentStore`](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore)\n",
        "- [`InMemoryBM25Retriever`](https://docs.haystack.deepset.ai/docs/inmemorybm25retriever)\n",
        "- DynamicChatPromptBuilder\n",
        "- [`FilterRetriever`](https://docs.haystack.deepset.ai/docs/filterretriever)\n",
        "- [`DocumentWriter`](https://docs.haystack.deepset.ai/docs/documentwriter)\n",
        "- [`OutputAdapter`](https://docs.haystack.deepset.ai/docs/outputadapter)\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "You'll need an [OpenAI API Key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key), although this code could be adapted to use [any model Haystack supports](https://docs.haystack.deepset.ai/docs/generators)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "aKuneM4lWtfv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQbU8GUfO-qZ",
        "outputId": "700106da-a7e1-4645-903d-a2854bb98003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting haystack-ai\n",
            "  Downloading haystack_ai-2.2.0-py3-none-any.whl (345 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.2/345.2 kB 4.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.1.4)\n",
            "Collecting lazy-imports (from haystack-ai)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from haystack-ai)\n",
            "  Downloading openai-1.31.1-py3-none-any.whl (324 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 324.1/324.1 kB 6.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.0.3)\n",
            "Collecting posthog (from haystack-ai)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.3/41.3 kB 3.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.31.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (8.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.66.4)\n",
            "Collecting trafilatura (from haystack-ai)\n",
            "  Downloading trafilatura-1.10.0-py3-none-any.whl (1.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 11.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.12.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.1.0->haystack-ai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.6/75.6 kB 5.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->haystack-ai) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog->haystack-ai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->haystack-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2024.6.2)\n",
            "Collecting courlan>=1.1.0 (from trafilatura->haystack-ai)\n",
            "  Downloading courlan-1.2.0-py3-none-any.whl (33 kB)\n",
            "Collecting htmldate>=1.8.1 (from trafilatura->haystack-ai)\n",
            "  Downloading htmldate-1.8.1-py3-none-any.whl (31 kB)\n",
            "Collecting justext>=3.0.1 (from trafilatura->haystack-ai)\n",
            "  Downloading jusText-3.0.1-py2.py3-none-any.whl (837 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 837.8/837.8 kB 11.9 MB/s eta 0:00:00\n",
            "Collecting lxml>=5.2.2 (from trafilatura->haystack-ai)\n",
            "  Downloading lxml-5.2.2-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 26.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai) (1.2.1)\n",
            "Requirement already satisfied: babel>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from courlan>=1.1.0->trafilatura->haystack-ai) (2.15.0)\n",
            "Collecting tld>=0.13 (from courlan>=1.1.0->trafilatura->haystack-ai)\n",
            "  Downloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 263.8/263.8 kB 27.1 MB/s eta 0:00:00\n",
            "Collecting dateparser>=1.1.2 (from htmldate>=1.8.1->trafilatura->haystack-ai)\n",
            "  Downloading dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 30.2 MB/s eta 0:00:00\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 7.9 MB/s eta 0:00:00\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 6.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: lxml[html_clean]>=4.4.2 in /usr/local/lib/python3.10/dist-packages (from justext>=3.0.1->trafilatura->haystack-ai) (4.9.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (2.18.4)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.8.1->trafilatura->haystack-ai) (2024.5.15)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.8.1->trafilatura->haystack-ai) (5.2)\n",
            "INFO: pip is looking at multiple versions of lxml[html-clean] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting lxml-html-clean (from lxml>=5.2.2->trafilatura->haystack-ai)\n",
            "  Downloading lxml_html_clean-0.1.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: monotonic, tld, lxml, lazy-imports, h11, backoff, posthog, lxml-html-clean, httpcore, dateparser, courlan, httpx, htmldate, openai, justext, trafilatura, haystack-ai\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.9.4\n",
            "    Uninstalling lxml-4.9.4:\n",
            "      Successfully uninstalled lxml-4.9.4\n",
            "Successfully installed backoff-2.2.1 courlan-1.2.0 dateparser-1.2.0 h11-0.14.0 haystack-ai-2.2.0 htmldate-1.8.1 httpcore-1.0.5 httpx-0.27.0 justext-3.0.1 lazy-imports-0.3.1 lxml-5.2.2 lxml-html-clean-0.1.1 monotonic-1.6 openai-1.31.1 posthog-3.5.0 tld-0.13 trafilatura-1.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: lxml 4.9.4 does not provide the extra 'html_clean'\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "pip install haystack-ai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authorization"
      ],
      "metadata": {
        "id": "Qpf00rOFW2a5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnaLI90LIhu2",
        "outputId": "37eccf3c-9348-419f-b1c3-fdabf6b57e2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the document store\n",
        "\n",
        "Write some documents to it, which you'll ask the agent to refer to later."
      ],
      "metadata": {
        "id": "0-lHLCeVZqcQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3q0vLO1GLNL",
        "outputId": "80121ef0-7fb6-430d-b85e-51b381460832"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from haystack import Document, Pipeline\n",
        "from haystack.components.builders import DynamicChatPromptBuilder\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "documents = [Document(content=\"There are over 7,000 languages spoken around the world today.\"),\n",
        "             Document(content=\"Chinese language boasts the highest number of native speakers.\"),\n",
        "\t\t\t       Document(content=\"Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.\"),\n",
        "\t\t\t       Document(content=\"In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.\")]\n",
        "document_store.write_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve previous chat messages\n",
        "\n",
        "`FilterRetriever` is a special retriever that retrieves documents based on the passed-in `filters` parameter. Here, it is used to retrieve documents that were previously stored in the `InMemoryDocumentStore`.\n",
        "\n",
        "Note that no filters are passed in to the `FilterRetriever`. If we had a huge list of chat messages, this could get quite slow! In a production scenario, you'd want to create a session ID for each chat session and pass that in as a filter."
      ],
      "metadata": {
        "id": "Er_i5ClSaCb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from haystack.components.retrievers import FilterRetriever\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack import Document\n",
        "from haystack.components.converters import OutputAdapter\n",
        "from typing import List\n",
        "from haystack.document_stores.types import DuplicatePolicy\n",
        "\n",
        "memory_store = InMemoryDocumentStore()\n",
        "memory_retriever = FilterRetriever(memory_store)\n",
        "# The same ChatMessage can't be stored multiple times in memory_store\n",
        "memory_writer = DocumentWriter(memory_store, policy=DuplicatePolicy.SKIP)\n"
      ],
      "metadata": {
        "id": "KxLgfi11Z3BT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save `ChatMessage` objects\n",
        "\n",
        "As the pipeline runs through multiple loops, you'll need to save previous chat messages to the document store.\n",
        "\n",
        "This is a utility function that takes a list of `ChatMessage` objects and turns them into Haystack documents. The `OutputAdapter` component makes the `chat_messages_to_docs` function fit smoothly into our pipeline, passing the input from the component at the end of the previous loop iteration into the next."
      ],
      "metadata": {
        "id": "jF4lvfzdbkvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_messages_to_docs(chat_messages: List[ChatMessage]):\n",
        "    return [Document(content=message.content) for message in chat_messages]\n",
        "\n",
        "output_adapter = OutputAdapter(template=\"{{ chat_messages | chat_messages_to_docs }}\", output_type=List[Document], custom_filters={\"chat_messages_to_docs\": chat_messages_to_docs})"
      ],
      "metadata": {
        "id": "7hq8rZMvbrBz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the pipeline"
      ],
      "metadata": {
        "id": "_xvET3_Md0B1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, this pipelines uses the `InMemoryBM25Retriever` to do keyword-based retrieval. In a production scenario where you're searching a vast number of documents, an `EmbeddingRetriever` would be faster and more accurate.\n",
        "\n",
        "The `DynamicChatPromptBuilder` takes 3 arguments:\n",
        "- _query_, the chat message from the current loop iteration\n",
        "- _documents_, which were written when we initialized the `DocumentStore`\n",
        "- _memories_, or previous chat messages"
      ],
      "metadata": {
        "id": "a2Pam3kdfbF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# components for RAG\n",
        "pipeline = Pipeline()\n",
        "pipeline.add_component(\"retriever\", InMemoryBM25Retriever(document_store=document_store))\n",
        "pipeline.add_component(\"prompt_builder\", DynamicChatPromptBuilder(runtime_variables=[\"query\", \"documents\", \"memories\"]))\n",
        "pipeline.add_component(\"llm\", OpenAIChatGenerator())\n",
        "\n",
        "# components for memory\n",
        "pipeline.add_component(\"memory_retriever\", memory_retriever)\n",
        "pipeline.add_component(\"memory_writer\", memory_writer)\n",
        "pipeline.add_component(\"output_adapter\", output_adapter)\n",
        "\n",
        "# connections for RAG\n",
        "pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
        "pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
        "\n",
        "# connections for memory\n",
        "pipeline.connect(\"memory_retriever\", \"prompt_builder.memories\")\n",
        "pipeline.connect(\"llm.replies\", \"output_adapter.chat_messages\")\n",
        "pipeline.connect(\"output_adapter\", \"memory_writer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Znc3HnwbdzTk",
        "outputId": "cc9d064b-d4fd-4cd5-b962-1371b6c74b11"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<haystack.core.pipeline.pipeline.Pipeline object at 0x7da851662bc0>\n",
              "🚅 Components\n",
              "  - retriever: InMemoryBM25Retriever\n",
              "  - prompt_builder: DynamicChatPromptBuilder\n",
              "  - llm: OpenAIChatGenerator\n",
              "  - memory_retriever: FilterRetriever\n",
              "  - memory_writer: DocumentWriter\n",
              "  - output_adapter: OutputAdapter\n",
              "🛤️ Connections\n",
              "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
              "  - prompt_builder.prompt -> llm.messages (List[ChatMessage])\n",
              "  - llm.replies -> output_adapter.chat_messages (List[ChatMessage])\n",
              "  - memory_retriever.documents -> prompt_builder.memories (List[Document])\n",
              "  - output_adapter.output -> memory_writer.documents (List[Document])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draw the pipeline\n",
        "\n",
        "If you want to see a diagram of the pipeline, running this cell will create a file locally."
      ],
      "metadata": {
        "id": "-rYd4QZ0j4E8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W2CSVnkwKQLJ"
      },
      "outputs": [],
      "source": [
        "pipeline.draw(path=\"./image.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the prompt\n",
        "\n",
        "The system message tells the LLM how to act.\n",
        "\n",
        "We need 2 loops here in our prompt: one to run through the memories, one to run through any relevant documents that were returned."
      ],
      "metadata": {
        "id": "PAREbRszj-29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = ChatMessage.from_system(\"You are a helpful assistant.\")\n",
        "user_message_template =\"\"\"Given the previous messages and the provided documents, answer the question. Use your memory.\n",
        "    Memory:\n",
        "    {% for memory in memories %}\n",
        "        {{ memory.content }}\n",
        "    {% endfor %}\n",
        "\n",
        "    Documents:\n",
        "    {% for doc in documents %}\n",
        "        {{ doc.content }}\n",
        "    {% endfor %}\n",
        "\n",
        "    \\nQuestion: {{query}}\n",
        "    \\nAnswer:\n",
        "\"\"\"\n",
        "user_message = ChatMessage.from_user(user_message_template)"
      ],
      "metadata": {
        "id": "aJW16ERZj3KB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bring it all together\n",
        "\n",
        "What follows is glue code to run the pipeline in a loop, provide input instructions, and break when the user enters `Q` to quit.\n",
        "\n",
        "Here are some example questions to get you started:\n",
        "- _How many languages are there?_\n",
        "- _What is the one with most native speakers_\n",
        "- _Do you remember the two answers you gave to me before?_"
      ],
      "metadata": {
        "id": "uFwkDCbAk-P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "while True:\n",
        "    messages = [system_message, user_message]\n",
        "    question = input(\"Enter your question or Q to exit. Example: How many languages are there?\\n🔮 \")\n",
        "    if question==\"Q\":\n",
        "        break\n",
        "\n",
        "    res = pipeline.run(data={\"retriever\": {\"query\": question}, \"prompt_builder\": {\"prompt_source\": messages, \"query\": question}}, include_outputs_from=[\"llm\", \"prompt_builder\"])\n",
        "    print(f\"res: {res}\")\n",
        "    assistant_resp = res['llm']['replies'][0]\n",
        "    print(f\"🤖 {assistant_resp.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk1ScpLGk-hS",
        "outputId": "86f95878-88db-4d64-803d-d47f1655278c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question or Q to exit. Example: How many languages are there?\n",
            "🔮 How many languages are there?\n",
            "res: {'memory_writer': {'documents_written': 1}, 'prompt_builder': {'prompt': [ChatMessage(content='You are a helpful assistant.', role=<ChatRole.SYSTEM: 'system'>, name=None, meta={}), ChatMessage(content='Given the previous messages and the provided documents, answer the question. Use your memory.\\n    Memory:\\n    \\n\\n    Documents:\\n    \\n        There are over 7,000 languages spoken around the world today.\\n    \\n        Chinese language boasts the highest number of native speakers.\\n    \\n        Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.\\n    \\n        In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.\\n    \\n\\n    \\nQuestion: How many languages are there?\\n    \\nAnswer:', role=<ChatRole.USER: 'user'>, name=None, meta={})]}, 'llm': {'replies': [ChatMessage(content='There are over 7,000 languages spoken around the world today.', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 14, 'prompt_tokens': 144, 'total_tokens': 158}})]}}\n",
            "🤖 There are over 7,000 languages spoken around the world today.\n",
            "Enter your question or Q to exit. Example: How many languages are there?\n",
            "🔮 What is the language with the most native speakers?\n",
            "res: {'memory_writer': {'documents_written': 1}, 'prompt_builder': {'prompt': [ChatMessage(content='You are a helpful assistant.', role=<ChatRole.SYSTEM: 'system'>, name=None, meta={}), ChatMessage(content='Given the previous messages and the provided documents, answer the question. Use your memory.\\n    Memory:\\n    \\n        There are over 7,000 languages spoken around the world today.\\n    \\n\\n    Documents:\\n    \\n        Chinese language boasts the highest number of native speakers.\\n    \\n        In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.\\n    \\n        There are over 7,000 languages spoken around the world today.\\n    \\n        Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.\\n    \\n\\n    \\nQuestion: What is the language with the most native speakers?\\n    \\nAnswer:', role=<ChatRole.USER: 'user'>, name=None, meta={})]}, 'llm': {'replies': [ChatMessage(content='Chinese language has the most native speakers.', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 8, 'prompt_tokens': 164, 'total_tokens': 172}})]}}\n",
            "🤖 Chinese language has the most native speakers.\n",
            "Enter your question or Q to exit. Example: How many languages are there?\n",
            "🔮 Do you remember the two answers you gave me before?\n",
            "res: {'memory_writer': {'documents_written': 1}, 'prompt_builder': {'prompt': [ChatMessage(content='You are a helpful assistant.', role=<ChatRole.SYSTEM: 'system'>, name=None, meta={}), ChatMessage(content='Given the previous messages and the provided documents, answer the question. Use your memory.\\n    Memory:\\n    \\n        There are over 7,000 languages spoken around the world today.\\n    \\n        Chinese language has the most native speakers.\\n    \\n\\n    Documents:\\n    \\n        In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.\\n    \\n        Chinese language boasts the highest number of native speakers.\\n    \\n        There are over 7,000 languages spoken around the world today.\\n    \\n        Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.\\n    \\n\\n    \\nQuestion: Do you remember the two answers you gave me before?\\n    \\nAnswer:', role=<ChatRole.USER: 'user'>, name=None, meta={})]}, 'llm': {'replies': [ChatMessage(content='The two answers I provided earlier were:\\n\\n1. There are over 7,000 languages spoken around the world today.\\n2. Chinese language has the most native speakers.', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 34, 'prompt_tokens': 175, 'total_tokens': 209}})]}}\n",
            "🤖 The two answers I provided earlier were:\n",
            "\n",
            "1. There are over 7,000 languages spoken around the world today.\n",
            "2. Chinese language has the most native speakers.\n",
            "Enter your question or Q to exit. Example: How many languages are there?\n",
            "🔮 Q\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}